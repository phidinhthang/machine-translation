{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cài đặt các thư viện sử dụng"
      ],
      "metadata": {
        "id": "_5qgDjjP6Xs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40a35WnRPqRF",
        "outputId": "45099a2a-7cde-46dc-e003-afdaaa29bfce",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting prenlp\n",
            "  Downloading prenlp-0.0.13-py3-none-any.whl (30 kB)\n",
            "Collecting nltk==3.2.5 (from prenlp)\n",
            "  Downloading nltk-3.2.5.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konlpy (from prenlp)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from prenlp)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ijson (from prenlp)\n",
            "  Downloading ijson-3.2.0.post0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.3/113.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py7zr==0.5b5 (from prenlp)\n",
            "  Downloading py7zr-0.5b5-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.2.5->prenlp) (1.16.0)\n",
            "Collecting texttable (from py7zr==0.5b5->prenlp)\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->prenlp)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy->prenlp) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy->prenlp) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy->prenlp) (23.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392143 sha256=e50ec45e1968d7194f8632ca40c9cd36f9f96a9a11318acde929751bb6867e04\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/d6/35/4a8a48ea9fe03abae30da7971b8ed2a350436bebc00541372b\n",
            "Successfully built nltk\n",
            "Installing collected packages: texttable, sentencepiece, ijson, py7zr, nltk, JPype1, konlpy, prenlp\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed JPype1-1.4.1 ijson-3.2.0.post0 konlpy-0.6.0 nltk-3.2.5 prenlp-0.0.13 py7zr-0.5b5 sentencepiece-0.1.99 texttable-1.6.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n"
          ]
        }
      ],
      "source": [
        "%pip install prenlp\n",
        "%pip install torch\n",
        "%pip install tqdm\n",
        "%pip install requests\n",
        "%pip install -q gradio\n",
        "!pip install prenlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mở 2 file chứa dataset vi_sents và en_sents, tạo một file mới en-to-vi.txt chứa dataset được xử lý từ 2 file trên"
      ],
      "metadata": {
        "id": "t2i3XzeT6hP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "BcTmPrGC4woS"
      },
      "outputs": [],
      "source": [
        "vi = open('./data/vi_sents')\n",
        "en = open('./data/en_sents')\n",
        "en2vi = open('./data/en-to-vi.txt', 'a+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1oOP7rNx4woT"
      },
      "outputs": [],
      "source": [
        "i = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nối từng dòng tương ứng của file vi_sents và file en_sents với nhau theo định dạng en_line => vi_line, ghi kết quả vào file en-to-vi.txt"
      ],
      "metadata": {
        "id": "w-VjSz3Z618_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "HviAXpZg4woT"
      },
      "outputs": [],
      "source": [
        "for vi_line, en_line in zip(vi, en):\n",
        "    en2vi.write(f'{en_line.strip()} => {vi_line.strip()}\\n')\n",
        "    i = i + 1\n",
        "    if i == 32768:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import các thư viện sử dụng"
      ],
      "metadata": {
        "id": "zucvFLKh7CHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIIJvJ6CPFVB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from collections import OrderedDict\n",
        "import zipfile\n",
        "import pathlib\n",
        "\n",
        "from prenlp.tokenizer import SentencePiece\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "tN14p6an7H_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OMlte0eeLFi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "no_cuda = False\n",
        "multi_gpu = False\n",
        "max_seq_len = 192\n",
        "d_model = 384\n",
        "n_layers = 12\n",
        "n_heads = 8\n",
        "dropout = 0.1\n",
        "d_ff = 1280\n",
        "batch_size = 128\n",
        "vocab_size = 10800"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXf7nG8k4woX"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() and not no_cuda else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Định nghĩa class Tokenizer"
      ],
      "metadata": {
        "id": "x35Pce0R7cSh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n78KtQOmPzsR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, tokenizer, vocab_file: str, \n",
        "               pad_token: str = '[PAD]',\n",
        "               unk_token: str = '[UNK]',\n",
        "               bos_token: str = '[BOS]',\n",
        "               eos_token: str = '[EOS]'):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.pad_token = pad_token\n",
        "    self.unk_token = unk_token\n",
        "    self.bos_token = bos_token\n",
        "    self.eos_token = eos_token\n",
        "    self.vocab = OrderedDict()\n",
        "    self.ids_to_tokens = OrderedDict()\n",
        "\n",
        "    with open(vocab_file, 'r', encoding='utf-8') as reader:\n",
        "      for i, line in enumerate(reader.readlines()):\n",
        "        token = line.split()[0]\n",
        "        self.vocab[token] = i\n",
        "\n",
        "    for token, id in self.vocab.items():\n",
        "      self.ids_to_tokens[id] = token\n",
        "\n",
        "  def tokenize(self, text: str) -> List[str]:\n",
        "    return self.tokenizer(text)\n",
        "\n",
        "  def convert_token_to_id(self, token: str) -> int:\n",
        "    return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "  def convert_id_to_token(self, id: int) -> str:\n",
        "    return self.ids_to_tokens(id, self.unk_token)\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
        "    return [self.convert_token_to_id(token) for token in tokens]\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n",
        "    return [self.convert_id_to_token(id) for id in ids]\n",
        "\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self) -> int:\n",
        "    return len(self.vocab)\n",
        "\n",
        "  @property\n",
        "  def pad_token_id(self) -> int:\n",
        "    return self.convert_token_to_id(self.pad_token)\n",
        "\n",
        "  @property\n",
        "  def unk_token_id(self) -> int:\n",
        "    return self.convert_token_to_id(self.unk_token)\n",
        "\n",
        "  @property\n",
        "  def bos_token_id(self) -> int:\n",
        "    return self.convert_token_to_id(self.bos_token)\n",
        "\n",
        "  @property\n",
        "  def eos_token_id(self) -> int:\n",
        "    return self.convert_token_to_id(self.eos_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Định nghĩa class PretrainedTokenizer, load tokenizer đã được huấn luyện từ trước"
      ],
      "metadata": {
        "id": "zaZNKVu37hpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7Cp08xCRb3G",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class PretrainedTokenizer(Tokenizer):\n",
        "  def __init__(self, pretrained_model: str, vocab_file: str,\n",
        "               pad_token: str = '[PAD]',\n",
        "               unk_token: str = '[UNK]',\n",
        "               bos_token: str = '[BOS]',\n",
        "               eos_token: str = '[EOS]'):\n",
        "    tokenizer = SentencePiece.load(pretrained_model)\n",
        "\n",
        "    super(PretrainedTokenizer, self).__init__(tokenizer, vocab_file, pad_token, unk_token, bos_token, eos_token)\n",
        "\n",
        "  def detokenize(self, tokens: List[str]) -> str:\n",
        "    return self.tokenizer.detokenize(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build tokenizer sử dụng thuật toán bpe trên tập dataset, output thu được lưu vào file tok.vocab và tok.model"
      ],
      "metadata": {
        "id": "7WlZiR-N7p0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5UTwRkdtLXb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "SentencePiece.train(input='./data/en-to-vi.txt', vocab_size=vocab_size, model_type='bpe', model_prefix='tok', max_sentence_length=max_seq_len + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load tokenizer từ file tok.model và tok.vocab"
      ],
      "metadata": {
        "id": "uxpnuOP675AC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUSi9eVVtV2Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenizer = PretrainedTokenizer(pretrained_model='tok.model', vocab_file='tok.vocab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xluAOgKMSsFt",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class InputFeatures:\n",
        "  def __init__(self, input_ids: List[int], output_ids: List[int]):\n",
        "    self.input_ids = input_ids\n",
        "    self.output_ids = output_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Biến đổi đoạn văn thô (từng dòng của file dataset) thành input có thể đưa vào mô hình"
      ],
      "metadata": {
        "id": "Gf68RWi28DaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGRg_uX6S9p7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def convert_texts_to_features(texts, tokenizer, max_seq_len):\n",
        "  pad_token_id = tokenizer.pad_token_id\n",
        "  bos_token_id = tokenizer.bos_token_id\n",
        "  eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "  features = []\n",
        "  for i, text in enumerate(texts):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    ids = [bos_token_id] + ids + [eos_token_id]\n",
        "\n",
        "    src_ids = ids[:max_seq_len]\n",
        "    tgt_ids = ids[1:max_seq_len + 1]\n",
        "\n",
        "    padding_length = max_seq_len - len(src_ids)\n",
        "    src_ids = src_ids + ([pad_token_id] * padding_length)\n",
        "\n",
        "    padding_length = max_seq_len - len(tgt_ids)\n",
        "    tgt_ids = tgt_ids + ([pad_token_id] * padding_length)\n",
        "\n",
        "    feature = InputFeatures(input_ids=src_ids,\n",
        "                            output_ids=tgt_ids)\n",
        "    \n",
        "    features.append(feature)\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7XoYGWgUJUA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def create_examples(dataset_src, max_seq_len, tokenizer, mode='train', split_ratio=0.1, random_seed=42):\n",
        "  random.seed(random_seed)\n",
        "\n",
        "  dataset = []\n",
        "  with open(dataset_src, 'r', encoding='utf-8') as reader:\n",
        "    for line in reader.readlines():\n",
        "      text = line.strip()\n",
        "      if text != '':\n",
        "        dataset.append(text)\n",
        "\n",
        "  random.shuffle(dataset)\n",
        "\n",
        "  if mode == 'train':\n",
        "    dataset = dataset[:int(len(dataset)*(1-split_ratio))]\n",
        "  elif mode == 'test':\n",
        "    dataset = dataset[int(len(dataset)*(1-split_ratio))]\n",
        "\n",
        "  features = convert_texts_to_features(dataset, tokenizer, max_seq_len)\n",
        "\n",
        "  input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "  output_ids = torch.tensor([feature.output_ids for feature in features], dtype=torch.long)\n",
        "\n",
        "  dataset = TensorDataset(input_ids, output_ids)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEJ9rUqat9QR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_dataset = create_examples(dataset_src='./data/en-to-vi.txt', max_seq_len=max_seq_len, mode='train', tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Định nghĩa các block trong mô hình Transformer"
      ],
      "metadata": {
        "id": "MXfn-uT28O2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEIFc5aRVgXb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  def __init__(self, d_k):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "    self.d_k = d_k\n",
        "\n",
        "  def forward(self, q, k, v, attn_mask):\n",
        "    attn_score = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.d_k)\n",
        "    attn_score.masked_fill_(attn_mask, -1e9)\n",
        "\n",
        "    attn_weights = nn.Softmax(dim=-1)(attn_score)\n",
        "\n",
        "    output = torch.matmul(attn_weights, v)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgitYwO-WyR4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_k = self.d_v = d_model//n_heads\n",
        "\n",
        "    self.WQ = nn.Linear(d_model, d_model)\n",
        "    self.WK = nn.Linear(d_model, d_model)\n",
        "    self.WV = nn.Linear(d_model, d_model)\n",
        "    self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k)\n",
        "    self.linear = nn.Linear(n_heads * self.d_v, d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, attn_mask):\n",
        "    batch_size = Q.size(0)\n",
        "\n",
        "    q_heads = self.WQ(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    k_heads = self.WK(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    v_heads = self.WV(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "    attn = self.scaled_dot_product_attn(q_heads, k_heads, v_heads, attn_mask)\n",
        "\n",
        "    attn = attn.transpose(1, 2).contiguous().view(batch_size, -1,self.n_heads * self.d_v)\n",
        "    output = self.linear(attn)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13C4wbR_YdXI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForwardNetwork(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(PositionWiseFeedForwardNetwork, self).__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    output = self.relu(self.linear1(inputs))\n",
        "    output = self.linear2(output)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIXiHq6ZcwjB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, p_drop, d_ff):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "    self.dropout1 = nn.Dropout(p_drop)\n",
        "    self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    self.ffn = PositionWiseFeedForwardNetwork(d_model, d_ff)\n",
        "    self.dropout2 = nn.Dropout(p_drop)\n",
        "    self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "  def forward(self, inputs, attn_mask):\n",
        "    attn_outputs = self.mha(inputs, inputs, inputs, attn_mask)\n",
        "    attn_outputs = self.dropout1(attn_outputs)\n",
        "    attn_outputs = self.layernorm1(inputs + attn_outputs)\n",
        "\n",
        "    ffn_outputs = self.ffn(attn_outputs)\n",
        "    ffn_outputs = self.dropout2(ffn_outputs)\n",
        "    ffn_outputs = self.layernorm2(attn_outputs + ffn_outputs)\n",
        "\n",
        "    return ffn_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNpkf7lJea6S",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, n_layers, n_heads, p_drop, d_ff, pad_id, sinusoid_table):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    self.pad_id = pad_id\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_embedding = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\n",
        "    self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, p_drop, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).repeat(inputs.size(0), 1) + 1\n",
        "    position_pad_mask = inputs.eq(self.pad_id)\n",
        "    positions.masked_fill_(position_pad_mask, 0)\n",
        "\n",
        "    outputs = self.embedding(inputs) + self.pos_embedding(positions)\n",
        "\n",
        "    attn_pad_mask = self.get_attention_padding_mask(inputs, inputs, self.pad_id)\n",
        "    attn_subsequent_mask = self.get_attention_subsequent_mask(inputs).to(device=attn_pad_mask.device)\n",
        "    attn_mask = torch.gt((attn_pad_mask.to(dtype=attn_subsequent_mask.dtype) + attn_subsequent_mask), 0)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      outputs = layer(outputs, attn_mask)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def get_attention_padding_mask(self, q, k, pad_id):\n",
        "    attn_pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
        "\n",
        "    return attn_pad_mask\n",
        "\n",
        "  def get_attention_subsequent_mask(self, q):\n",
        "    bs, q_len = q.size()\n",
        "    subsequent_mask = torch.ones(bs, q_len, q_len).triu(diagonal=1)\n",
        "\n",
        "    return subsequent_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pslKVtLkkhkn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               seq_len,\n",
        "               d_model=512,\n",
        "               n_layers=6,\n",
        "               n_heads=8,\n",
        "               p_drop=0.1,\n",
        "               d_ff=2048,\n",
        "               pad_id=0):\n",
        "    super(Transformer, self).__init__()\n",
        "    sinusoid_table = self.get_sinusoid_table(seq_len+1, d_model)\n",
        "\n",
        "    self.decoder = TransformerDecoder(vocab_size, d_model, n_layers, n_heads, p_drop, d_ff, pad_id, sinusoid_table)\n",
        "    self.linear = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    decoder_outputs = self.decoder(inputs)\n",
        "\n",
        "    outputs = self.linear(decoder_outputs)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "  def get_sinusoid_table(self, seq_len, d_model):\n",
        "    def get_angle(pos, i, d_model):\n",
        "      return pos / np.power(10000, (2 * (i//2) / d_model))\n",
        "\n",
        "    sinusoid_table = np.zeros((seq_len, d_model))\n",
        "    for pos in range(seq_len):\n",
        "      for i in range(d_model):\n",
        "        if i % 2 == 0:\n",
        "          sinusoid_table[pos, i] = np.sin(get_angle(pos, i, d_model))\n",
        "        else:\n",
        "          sinusoid_table[pos, i] = np.cos(get_angle(pos, i, d_model))\n",
        "\n",
        "    return torch.FloatTensor(sinusoid_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v97ShgIgHcMX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = Transformer(vocab_size=vocab_size, seq_len=max_seq_len, d_model=d_model, n_layers=n_layers, n_heads=n_heads, p_drop=dropout, d_ff=d_ff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFCeLtGC4wot",
        "outputId": "a4417a07-a60a-4375-edb9-5f494fb6bf00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (decoder): TransformerDecoder(\n",
              "    (embedding): Embedding(10800, 384)\n",
              "    (pos_embedding): Embedding(193, 384)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x DecoderLayer(\n",
              "        (mha): MultiHeadAttention(\n",
              "          (WQ): Linear(in_features=384, out_features=384, bias=True)\n",
              "          (WK): Linear(in_features=384, out_features=384, bias=True)\n",
              "          (WV): Linear(in_features=384, out_features=384, bias=True)\n",
              "          (scaled_dot_product_attn): ScaledDotProductAttention()\n",
              "          (linear): Linear(in_features=384, out_features=384, bias=True)\n",
              "        )\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (layernorm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "        (ffn): PositionWiseFeedForwardNetwork(\n",
              "          (linear1): Linear(in_features=384, out_features=1280, bias=True)\n",
              "          (linear2): Linear(in_features=1280, out_features=384, bias=True)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (layernorm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=384, out_features=10800, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "if multi_gpu:\n",
        "  model = nn.DataParallel(model)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9NnMb5k4wot"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0PAcS07FkdM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_t5qOeBH3At",
        "outputId": "7d702116-ce63-4ae5-a441-97ea05908d7f",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27236400"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tạo class optimizer"
      ],
      "metadata": {
        "id": "xsKTepqQ8YLM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHzQSUGhAgKf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class ScheduledOptim:\n",
        "  def __init__(self, optimizer, init_lr, d_model, n_warmup_steps=2000):\n",
        "    self.optimizer = optimizer\n",
        "    self.init_lr = init_lr\n",
        "    self.d_model = d_model\n",
        "    self.n_warmup_steps = n_warmup_steps\n",
        "    self.n_steps = 0\n",
        "    self.current_lr = init_lr\n",
        "\n",
        "  def _get_lr_scale(self):\n",
        "    return (self.d_model ** -0.5) * min(self.n_steps ** -0.5, self.n_steps * (self.n_warmup_steps ** -1.5))\n",
        "\n",
        "  def update_learning_rate(self):\n",
        "    self.n_steps += 1\n",
        "    self.current_lr = self.init_lr * self._get_lr_scale()\n",
        "    for param_group in self.optimizer.param_groups:\n",
        "      param_group['lr'] = self.current_lr\n",
        "    \n",
        "  def state_dict(self):\n",
        "    return self.optimizer.state_dict()\n",
        "\n",
        "  def load_state_dict(self, state_dict, n_steps: int):\n",
        "    self.optimizer.load_state_dict(state_dict)\n",
        "    self.n_steps = n_steps\n",
        "    self.update_learning_rate()\n",
        "\n",
        "  def zero_grad(self):\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "    \n",
        "  def get_n_steps(self):\n",
        "    return self.n_steps\n",
        "\n",
        "  @property\n",
        "  def get_current_lr(self):\n",
        "    return self.current_lr "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "kQCU6FCm4wpD"
      },
      "outputs": [],
      "source": [
        "optimizer = ScheduledOptim(optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9), init_lr=2.0, d_model=d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tạo class checkpoint dùng để lưu tiến trình huấn luyện, trong đó gồm có tham số đã được huấn luyện"
      ],
      "metadata": {
        "id": "F1ZfIOPo8bW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhpvk9FN4wpE"
      },
      "outputs": [],
      "source": [
        "class Checkpoint:\n",
        "  def __init__(self, net: nn.Module, optimizer: ScheduledOptim, version: int, subdir='checkpoint'):\n",
        "    self.subdir = subdir\n",
        "    self.net = net\n",
        "    self.optimizer = optimizer\n",
        "    self.version = version\n",
        "    self.path = self.get_path()\n",
        "\n",
        "    if not os.path.exists(subdir):\n",
        "      os.makedirs(subdir)\n",
        "\n",
        "  def save(self):\n",
        "    print(f'current_version:{self.version}, n_steps:{self.optimizer.get_n_steps()}')\n",
        "    torch.save({\n",
        "        'version': self.version,\n",
        "        'model_state_dict': self.net.state_dict(),\n",
        "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "        'n_steps': self.optimizer.get_n_steps()\n",
        "    }, self.path)\n",
        "    self.version = self.version + 1\n",
        "    self.path = self.get_path()\n",
        "\n",
        "  def get_path(self):\n",
        "    return f'{self.subdir}/epoch-{self.version}.pt'\n",
        "\n",
        "  def load(self):\n",
        "    checkpoint = torch.load(self.path)\n",
        "    print('n_steps:', checkpoint['n_steps'])\n",
        "    print('version:', checkpoint['version'])\n",
        "    self.net.load_state_dict(checkpoint['model_state_dict'])\n",
        "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'], checkpoint['n_steps'])\n",
        "    self.version = self.version + 1\n",
        "    self.path = self.get_path()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJA3iqH14wpF"
      },
      "outputs": [],
      "source": [
        "checkpoint_version = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQDMkex94wpG"
      },
      "outputs": [],
      "source": [
        "checkpoint = Checkpoint(net=model, optimizer=optimizer, version=checkpoint_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load checkpoint để retrain hoặc inference"
      ],
      "metadata": {
        "id": "3vEv6dVC8k5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_i3d7LY4wpH",
        "outputId": "e3294847-11cc-4c46-a037-366130e6f3dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_steps: 9200\n",
            "version: 40\n"
          ]
        }
      ],
      "source": [
        "checkpoint.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "H7B1DyKw8rqI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngY6hOjBDblA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "  def __init__(self, \n",
        "               train_loader, \n",
        "               tokenizer, \n",
        "               optimizer,\n",
        "               model):  \n",
        "    self.train_loader = train_loader\n",
        "    self.vocab_size = tokenizer.vocab_size\n",
        "    self.pad_id = tokenizer.pad_token_id\n",
        "\n",
        "    self.model = model\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_id)\n",
        "\n",
        "  def train(self, epoch):\n",
        "    losses = 0\n",
        "    n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset)\n",
        "\n",
        "    self.model.train()\n",
        "    for i, batch in enumerate(self.train_loader):\n",
        "      start_time = time.time()\n",
        "      inputs, ground_truth_outputs = map(lambda x: x.to(device), batch)\n",
        "      outputs = self.model(inputs)\n",
        "\n",
        "      loss = self.criterion(outputs.view(-1, self.vocab_size), ground_truth_outputs.view(-1))\n",
        "      losses += loss.item()\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.update_learning_rate()\n",
        "      self.optimizer.step()\n",
        "    \n",
        "      end_time = time.time()\n",
        "\n",
        "\n",
        "      if i % 20 == 0:\n",
        "        print('Iteration {} ({}/{})\\tLoss: {:.4f}\\tlr: {:.4f}\\tduration: {:.4f}'.format(i, i, n_batches, losses/(i + 1), self.optimizer.get_current_lr, end_time - start_time))\n",
        "\n",
        "    checkpoint.save()\n",
        "    print('Train Epoch: {}\\t\\tLoss: {:.4f}\\tnum_steps: {}'.format(epoch, losses/n_batches, self.optimizer.get_n_steps()))\n",
        "\n",
        "  def save(self, epoch, model_prefix='model', root='./model'):\n",
        "    path = Path(root) / (model_prefix + '.ep%d' % epoch)\n",
        "    if not path.parent.exists():\n",
        "      path.parent.mkdir()\n",
        "\n",
        "    torch.save(self.model, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVM0aTcXrWQr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZn2XtswrujQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(tokenizer=tokenizer, train_loader=train_loader, optimizer=optimizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6pVqtL4sNmp",
        "outputId": "413615f1-099f-4706-e0b5-f948537af42c",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 (0/231)\tLoss: 0.7363\tlr: 0.0011\tduration: 1.2512\n",
            "Iteration 20 (20/231)\tLoss: 0.7491\tlr: 0.0011\tduration: 1.2621\n",
            "Iteration 40 (40/231)\tLoss: 0.7494\tlr: 0.0011\tduration: 1.2802\n",
            "Iteration 60 (60/231)\tLoss: 0.7481\tlr: 0.0011\tduration: 1.2400\n",
            "Iteration 80 (80/231)\tLoss: 0.7560\tlr: 0.0011\tduration: 1.2429\n",
            "Iteration 100 (100/231)\tLoss: 0.7676\tlr: 0.0011\tduration: 1.2632\n",
            "Iteration 120 (120/231)\tLoss: 0.7798\tlr: 0.0011\tduration: 1.2498\n",
            "Iteration 140 (140/231)\tLoss: 0.7862\tlr: 0.0011\tduration: 1.2495\n",
            "Iteration 160 (160/231)\tLoss: 0.7918\tlr: 0.0011\tduration: 1.2547\n",
            "Iteration 180 (180/231)\tLoss: 0.7973\tlr: 0.0011\tduration: 1.2531\n",
            "Iteration 200 (200/231)\tLoss: 0.8020\tlr: 0.0011\tduration: 1.2528\n",
            "Iteration 220 (220/231)\tLoss: 0.8076\tlr: 0.0011\tduration: 1.2516\n",
            "current_version:41, n_steps:9432\n",
            "Train Epoch: 1\t\tLoss: 0.8101\tnum_steps: 9432\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, 40 + 1):\n",
        "  trainer.train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzvLxnPLHPxT"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quM_aukZHmTN",
        "tags": []
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "OAYEDLzb8w0d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKPH73-kvuJd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text):\n",
        "  text = text + ' =>'\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  ids = [tokenizer.bos_token_id] + tokenizer.convert_tokens_to_ids(tokens)\n",
        "  ids = ids[:max_seq_len]\n",
        "  input_length = len(ids)\n",
        "  translate = []\n",
        "  for i in range(max_seq_len - input_length):\n",
        "    inputs = torch.tensor(ids).unsqueeze(0).to(device)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    output_token_id = outputs[:, -1, :].argmax(dim=-1).item()\n",
        "    if output_token_id == tokenizer.eos_token_id:\n",
        "      break\n",
        "    else:\n",
        "      ids = ids + [output_token_id]\n",
        "      translate = translate + [output_token_id]\n",
        "\n",
        "  translate = list(map(lambda id: tokenizer.ids_to_tokens[id], translate))\n",
        "\n",
        "  translate = list(map(lambda x: x.replace('▁', ' '), translate))\n",
        "\n",
        "  translate = ''.join(translate)\n",
        "\n",
        "  return translate"
      ],
      "metadata": {
        "id": "Jzh4elT1B7ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xây dựng GUI cho mô hình"
      ],
      "metadata": {
        "id": "hKHkE1Qd8y1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradio.Interface(inference, \"text\", \"text\").launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "VHTarqA6w_dj",
        "outputId": "e7441de5-0217-4c2a-fd6d-1ea2a0e17d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://23b359f0bae0449b48.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://23b359f0bae0449b48.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}